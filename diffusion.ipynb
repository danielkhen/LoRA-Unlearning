{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "044aa744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "TORCH_SEED = 69\n",
    "torch.manual_seed(TORCH_SEED)\n",
    "\n",
    "import logging\n",
    "import os\n",
    "\n",
    "os.makedirs('weights', exist_ok=True)\n",
    "run_logger = logging.getLogger('run_logger')\n",
    "file_handler = logging.FileHandler('weights/runs.log')\n",
    "run_logger.addHandler(file_handler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c11e463",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "NUM_WORKERS = 8\n",
    "BATCH_SIZE = 128\n",
    "INPUT_SIZE = 64\n",
    "NUM_CLASSES = 10\n",
    "dataset = datasets.CIFAR10\n",
    "\n",
    "AUGMENTATIONS = (\n",
    "        transforms.Resize(INPUT_SIZE + INPUT_SIZE // 4),\n",
    "        transforms.RandomResizedCrop(INPUT_SIZE, scale=(0.8, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    ")\n",
    "\n",
    "NORMALIZATIONS = (\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize( (0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    ")\n",
    "\n",
    "# Lambda can't be pickled\n",
    "def one_hot_transform(label):\n",
    "    return one_hot(torch.tensor(label), NUM_CLASSES).squeeze()\n",
    "\n",
    "TARGET_TRANSFORM = transforms.Compose([\n",
    "        transforms.Lambda(one_hot_transform),\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "training_data = dataset(root='./data', train=True, download=True, transform=transforms.Compose(AUGMENTATIONS + NORMALIZATIONS), target_transform=TARGET_TRANSFORM)\n",
    "\n",
    "# Load data loaders\n",
    "train_dataloader = DataLoader(training_data, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f97ce5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_factory\n",
    "\n",
    "MODEL_NAME = 'context-unet'\n",
    "\n",
    "# Load model\n",
    "model = model_factory.create_model(MODEL_NAME, NUM_CLASSES, input_size=INPUT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2800988c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (inc): DoubleConv(\n",
       "    (double_conv): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (down1): Down(\n",
       "    (maxpool_conv): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "        )\n",
       "      )\n",
       "      (2): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (emb_layer): Sequential(\n",
       "      (0): SiLU()\n",
       "      (1): Linear(in_features=256, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (sa1): SelfAttention(\n",
       "    (mha): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (ff_self): Sequential(\n",
       "      (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (down2): Down(\n",
       "    (maxpool_conv): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "        )\n",
       "      )\n",
       "      (2): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (emb_layer): Sequential(\n",
       "      (0): SiLU()\n",
       "      (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (sa2): SelfAttention(\n",
       "    (mha): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (ff_self): Sequential(\n",
       "      (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (down3): Down(\n",
       "    (maxpool_conv): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
       "        )\n",
       "      )\n",
       "      (2): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (emb_layer): Sequential(\n",
       "      (0): SiLU()\n",
       "      (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (sa3): SelfAttention(\n",
       "    (mha): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (ln): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (ff_self): Sequential(\n",
       "      (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (bot1): DoubleConv(\n",
       "    (double_conv): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): GroupNorm(1, 512, eps=1e-05, affine=True)\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): GroupNorm(1, 512, eps=1e-05, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (bot2): DoubleConv(\n",
       "    (double_conv): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): GroupNorm(1, 512, eps=1e-05, affine=True)\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): GroupNorm(1, 512, eps=1e-05, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (bot3): DoubleConv(\n",
       "    (double_conv): Sequential(\n",
       "      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (up1): Up(\n",
       "    (up): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "    (conv): Sequential(\n",
       "      (0): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): GroupNorm(1, 512, eps=1e-05, affine=True)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): GroupNorm(1, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (emb_layer): Sequential(\n",
       "      (0): SiLU()\n",
       "      (1): Linear(in_features=256, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (sa4): SelfAttention(\n",
       "    (mha): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (ff_self): Sequential(\n",
       "      (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (up2): Up(\n",
       "    (up): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "    (conv): Sequential(\n",
       "      (0): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (emb_layer): Sequential(\n",
       "      (0): SiLU()\n",
       "      (1): Linear(in_features=256, out_features=64, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (sa5): SelfAttention(\n",
       "    (mha): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (ff_self): Sequential(\n",
       "      (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (1): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (up3): Up(\n",
       "    (up): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "    (conv): Sequential(\n",
       "      (0): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (emb_layer): Sequential(\n",
       "      (0): SiLU()\n",
       "      (1): Linear(in_features=256, out_features=64, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (sa6): SelfAttention(\n",
       "    (mha): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (ff_self): Sequential(\n",
       "      (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (1): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (outc): Conv2d(64, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (label_emb): Embedding(10, 256)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5165a821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fine_tuning\n",
    "\n",
    "LORA_RANK = 16\n",
    "LORA_TARGET_REGEXES = ['up0.*']\n",
    "\n",
    "# Apply LoRa\n",
    "model = fine_tuning.get_lora_model(model, LORA_RANK, LORA_TARGET_REGEXES)\n",
    "trainable_params = sum(param.numel() for param in model.parameters() if param.requires_grad)\n",
    "print(f\"Number of trainable parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19a7620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from save file\n",
    "SAVE_FILE = 'weights/model.pth'\n",
    "model = torch.load(SAVE_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42ed156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge and unload LoRa\n",
    "model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad77285",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim, nn\n",
    "\n",
    "LEARNING_RATE = 3e-4\n",
    "WEIGHT_DECAY = 0\n",
    "BETA1 = 1e-4\n",
    "BETA2 = 0.02\n",
    "TIMESTEPS = 1000\n",
    "\n",
    "# Load optimizer and criterion\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "b_t = torch.linspace(BETA1, BETA2, TIMESTEPS + 1).to('cuda')\n",
    "a_t = 1 - b_t\n",
    "ab_t = torch.cumprod(a_t, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caac209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from tqdm import tqdm\n",
    "\n",
    "SAVE_FILE = 'weights/model.pth'\n",
    "EPOCHS = 50\n",
    "EVAL_EVERY = 5\n",
    "LABELS_MASK_P = 0.9\n",
    "\n",
    "lr_scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "model.to('cuda')\n",
    "model.train()\n",
    "\n",
    "test_loss = None\n",
    "test_accuracy = None\n",
    "\n",
    "# Normal training\n",
    "with tqdm(range(EPOCHS), unit='epoch', desc='lr=None, train_loss=None') as pbar:\n",
    "    for epoch in pbar:\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, labels in train_dataloader:\n",
    "            # Move inputs and labels to the specified device\n",
    "            inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "\n",
    "            # perturb data\n",
    "            noise = torch.randn_like(inputs, device='cuda')\n",
    "            t = torch.randint(1, TIMESTEPS + 1, (inputs.shape[0], ), device='cuda')\n",
    "            sqrt_ab_t = ab_t[t].sqrt().view(-1, 1, 1, 1)\n",
    "            sqrt_one_minus_ab_t = (1 - ab_t[t]).sqrt().view(-1, 1, 1, 1)\n",
    "            x_pert = sqrt_ab_t * inputs + sqrt_one_minus_ab_t * noise\n",
    "\n",
    "            # Randomly mask labels\n",
    "            labels = labels*torch.bernoulli(torch.ones((labels.shape[0], 1), device='cuda')*LABELS_MASK_P)\n",
    "\n",
    "            # Predict noise\n",
    "            outputs = model(x_pert, t / TIMESTEPS, labels)\n",
    "\n",
    "            # Compute the loss and its gradients\n",
    "            loss = criterion(outputs, noise)\n",
    "            running_loss += loss.item()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Adjust learning weights and zero gradients\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        train_loss = running_loss / len(train_dataloader)\n",
    "\n",
    "        lr_scheduler.step()\n",
    "        lr = lr_scheduler.get_last_lr()[0]\n",
    "\n",
    "        if (epoch + 1) % EVAL_EVERY == 0:\n",
    "            run_logger.debug(f'Normal training checkpoint: save_file={SAVE_FILE}, epoch={epoch}, train_loss={train_loss}')\n",
    "            torch.save(model, f'{SAVE_FILE}.checkpoint')\n",
    "\n",
    "        pbar.set_postfix(lr=lr, train_loss=train_loss)\n",
    "\n",
    "run_logger.info(f'Normal training complete: save_file={SAVE_FILE}, train_loss={train_loss}')\n",
    "torch.save(model, SAVE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a958f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Number of samples to generate\n",
    "num_samples = 8\n",
    "\n",
    "# Sampling parameters\n",
    "with torch.no_grad():\n",
    "    # Start from pure noise\n",
    "    x = torch.randn(num_samples, 3, INPUT_SIZE, INPUT_SIZE).to('cuda')\n",
    "    labels = torch.zeros(num_samples, NUM_CLASSES).to('cuda')  # or use one-hot for conditional generation\n",
    "\n",
    "    for t in reversed(range(1, TIMESTEPS + 1)):\n",
    "        t_tensor = torch.full((num_samples,), t, device='cuda', dtype=torch.long)\n",
    "        t_norm = t_tensor / TIMESTEPS\n",
    "\n",
    "        # Predict noise\n",
    "        pred_noise = model(x, t_norm, labels)\n",
    "\n",
    "        # Compute coefficients\n",
    "        beta_t = b_t[t].view(-1, 1, 1, 1)\n",
    "        alpha_t = a_t[t].view(-1, 1, 1, 1)\n",
    "        ab_t_ = ab_t[t].view(-1, 1, 1, 1)\n",
    "\n",
    "        # DDPM sampling step\n",
    "        if t > 1:\n",
    "            noise = torch.randn_like(x)\n",
    "        else:\n",
    "            noise = torch.zeros_like(x)\n",
    "        x = (1 / alpha_t.sqrt()) * (x - (beta_t / (1 - ab_t_).sqrt()) * pred_noise) + beta_t.sqrt() * noise\n",
    "\n",
    "    # Convert to numpy and denormalize for visualization\n",
    "    samples = x.clamp(-1, 1).cpu().numpy()\n",
    "    samples = (samples * 0.5) + 0.5  # if your data was normalized to [-1, 1]\n",
    "\n",
    "    # Plot samples\n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(num_samples * 2, 2))\n",
    "    for i, ax in enumerate(axes):\n",
    "        img = np.transpose(samples[i], (1, 2, 0))\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75640ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "FORGET_SET_SIZE = 10000\n",
    "\n",
    "training_data = dataset(root='./data', train=True, download=True, transform=transforms.Compose(NORMALIZATIONS))\n",
    "\n",
    "# Class forgetting\n",
    "original_trainset = training_data\n",
    "unlearning_data = Subset(original_trainset, range(FORGET_SET_SIZE))\n",
    "training_data = Subset(original_trainset, range(FORGET_SET_SIZE, len(original_trainset)))\n",
    "\n",
    "# Load data loaders\n",
    "train_dataloader = DataLoader(training_data, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True)\n",
    "unlearn_dataloader = DataLoader(unlearning_data, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455edca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import eval\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "SAVE_FILE = 'weights/modelNG10000.pth'\n",
    "EPOCHS = 200\n",
    "EVAL_EVERY = 5\n",
    "UNLEAN_GRAD_METHOD = 'both' # 'both' or 'ascent\n",
    "\n",
    "model.to('cuda')\n",
    "model.train()\n",
    "\n",
    "test_loss = None\n",
    "test_accuracy = None\n",
    "\n",
    "# NegGrad unlearning\n",
    "with tqdm(range(EPOCHS), unit='epoch', desc='retain_accuracy=None, forget_accuracy=None, test_accuracy=None, retain_loss=None, forget_loss=None, test_loss=None') as pbar:\n",
    "    for epoch in pbar:\n",
    "        retain_running_loss, forget_running_loss = 0.0, 0.0\n",
    "        retain_correct_predictions, forget_correct_predictions = 0, 0\n",
    "        retain_total_predictions, forget_total_predictions = 0, 0\n",
    "\n",
    "        for (retain_inputs, retain_labels), (forget_inputs, forget_labels) in zip(train_dataloader, unlearn_dataloader):\n",
    "            # Move inputs and labels to the specified device\n",
    "            retain_inputs, retain_labels = retain_inputs.to('cuda'), retain_labels.to('cuda')\n",
    "            forget_inputs, forget_labels = forget_inputs.to('cuda'), forget_labels.to('cuda')\n",
    "\n",
    "            # Compute predictions\n",
    "            retain_outputs = model(retain_inputs)\n",
    "            forget_outputs = model(forget_inputs)\n",
    "            _, retain_predictions = torch.max(retain_outputs.data, 1)\n",
    "            _, forget_predictions = torch.max(forget_outputs.data, 1)\n",
    "\n",
    "            # Update the running total of correct predictions and samples\n",
    "            retain_correct_predictions += (retain_predictions == retain_labels).sum().item()\n",
    "            forget_correct_predictions += (forget_predictions == forget_labels).sum().item()\n",
    "            retain_total_predictions += retain_labels.size(0)\n",
    "            forget_total_predictions += forget_labels.size(0)\n",
    "\n",
    "            # Compute the loss and its gradients\n",
    "            retain_loss = criterion(retain_outputs, retain_labels)\n",
    "            forget_loss = criterion(forget_outputs, forget_labels)\n",
    "            retain_running_loss += retain_loss.item()\n",
    "            forget_running_loss += forget_loss.item()\n",
    "            (-1.0 * forget_loss).backward()\n",
    "\n",
    "            if UNLEAN_GRAD_METHOD == 'both':\n",
    "                retain_loss.backward()\n",
    "\n",
    "            # Adjust learning weights and zero gradients\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Calculate the average loss and accuracy\n",
    "        retain_loss = retain_running_loss / len(train_dataloader)\n",
    "        forget_loss = forget_running_loss / len(train_dataloader)\n",
    "        retain_accuracy = 100 * retain_correct_predictions / retain_total_predictions\n",
    "        forget_accuracy = 100 * forget_correct_predictions / forget_total_predictions\n",
    "        \n",
    "        test_loss, test_accuracy = eval.test(model, test_dataloader, criterion)\n",
    "\n",
    "        pbar.set_postfix(test_loss=test_loss, forget_loss=forget_loss, retain_loss=retain_loss, test_accuracy=test_accuracy, forget_accuracy=forget_accuracy, retain_accuracy=retain_accuracy)\n",
    "\n",
    "        if test_loss > forget_loss:\n",
    "            break\n",
    "\n",
    "run_logger.info(f'NegGrad unlearning complete: save_file={SAVE_FILE}, train_loss={train_loss}, train_accuracy={train_accuracy}, test_loss={test_loss}, test_accuracy={test_accuracy}')\n",
    "torch.save(model, SAVE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b161eb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import eval\n",
    "\n",
    "from orthograd import OrthogonalGrad, AdamUpdateDirection\n",
    "from tqdm import tqdm\n",
    "\n",
    "SAVE_FILE = 'weights/modelOG10000.pth'\n",
    "EPOCHS = 200\n",
    "EVAL_EVERY = 5\n",
    "\n",
    "model.to('cuda')\n",
    "model.train()\n",
    "\n",
    "test_loss = None\n",
    "test_accuracy = None\n",
    "\n",
    "optimizer_retain = AdamUpdateDirection(model.parameters())\n",
    "optimizer_forget = AdamUpdateDirection(model.parameters())\n",
    "\n",
    "unlearn_method = OrthogonalGrad(\n",
    "    lr=LEARNING_RATE,\n",
    "    loss=criterion,\n",
    "    optimizer_retain=optimizer_retain,\n",
    "    optimizer_unlearn=optimizer_forget,\n",
    "    retain_grad_mode='per_sample',\n",
    "    update_mode='both',\n",
    "    original_model=model,\n",
    "    grad_mask=None,\n",
    "    alpha=0.5,\n",
    ")\n",
    "\n",
    "# NegGrad unlearning\n",
    "with tqdm(range(EPOCHS), unit='epoch', desc='retain_accuracy=None, forget_accuracy=None, test_accuracy=None, retain_loss=None, forget_loss=None, test_loss=None') as pbar:\n",
    "    for epoch in pbar:\n",
    "        retain_running_loss, forget_running_loss = 0.0, 0.0\n",
    "        retain_correct_predictions, forget_correct_predictions = 0, 0\n",
    "        retain_total_predictions, forget_total_predictions = 0, 0\n",
    "        model.train() # Set the model to training mode\n",
    "\n",
    "        for (retain_inputs, retain_labels), (forget_inputs, forget_labels) in zip(train_dataloader, unlearn_dataloader):\n",
    "            # Move inputs and labels to the specified device\n",
    "            retain_inputs, retain_labels = retain_inputs.to('cuda'), retain_labels.to('cuda')\n",
    "            forget_inputs, forget_labels = forget_inputs.to('cuda'), forget_labels.to('cuda')\n",
    "\n",
    "            # Compute predictions\n",
    "            retain_outputs = model(retain_inputs)\n",
    "            forget_outputs = model(forget_inputs)\n",
    "            _, retain_predictions = torch.max(retain_outputs.data, 1)\n",
    "            _, forget_predictions = torch.max(forget_outputs.data, 1)\n",
    "\n",
    "            # Update the running total of correct predictions and samples\n",
    "            retain_correct_predictions += (retain_predictions == retain_labels).sum().item()\n",
    "            forget_correct_predictions += (forget_predictions == forget_labels).sum().item()\n",
    "            retain_total_predictions += retain_labels.size(0)\n",
    "            forget_total_predictions += forget_labels.size(0)\n",
    "\n",
    "            # Compute the loss and its gradients\n",
    "            forget_loss, retain_loss = unlearn_method(\n",
    "                model, forget_inputs, forget_labels, retain_inputs, retain_labels\n",
    "            )\n",
    "\n",
    "            retain_running_loss += retain_loss.item()\n",
    "            forget_running_loss += forget_loss.item()\n",
    "\n",
    "        # Calculate the average loss and accuracy\n",
    "        retain_avg_loss = retain_running_loss / len(train_dataloader)\n",
    "        forget_avg_loss = forget_running_loss / len(train_dataloader)\n",
    "        retain_accuracy = 100 * retain_correct_predictions / retain_total_predictions\n",
    "        forget_accuracy = 100 * forget_correct_predictions / forget_total_predictions\n",
    "        \n",
    "        test_loss, test_accuracy = eval.test(model, test_dataloader, criterion)\n",
    "\n",
    "        pbar.set_postfix(test_loss=test_loss, forget_loss=forget_loss, retain_loss=retain_loss, test_accuracy=test_accuracy, forget_accuracy=forget_accuracy, retain_accuracy=retain_accuracy)\n",
    "        \n",
    "        if test_loss > forget_loss:\n",
    "            break\n",
    "\n",
    "run_logger.info(f'OrthoGrad unlearning complete: save_file={SAVE_FILE}, train_loss={train_loss}, train_accuracy={train_accuracy}, test_loss={test_loss}, test_accuracy={test_accuracy}')\n",
    "torch.save(model, SAVE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb13f8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import eval\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "from gdr_gma import MemoryBank, get_gradient, rectify_graident\n",
    "\n",
    "SAVE_FILE = 'weights/modelGG10000.pth'\n",
    "EPOCHS = 200\n",
    "EVAL_EVERY = 5\n",
    "\n",
    "model.to('cuda')\n",
    "model.train()\n",
    "\n",
    "test_loss = None\n",
    "test_accuracy = None\n",
    "\n",
    "batches = math.ceil(len(zip(train_dataloader, unlearn_dataloader)) / BATCH_SIZE)\n",
    "bank = MemoryBank(size=batches)\n",
    "\n",
    "# NegGrad unlearning\n",
    "with tqdm(range(EPOCHS), unit='epoch', desc='retain_accuracy=None, forget_accuracy=None, test_accuracy=None, retain_loss=None, forget_loss=None, test_loss=None') as pbar:\n",
    "    for epoch in pbar:\n",
    "        retain_running_loss, forget_running_loss = 0.0, 0.0\n",
    "        retain_correct_predictions, forget_correct_predictions = 0, 0\n",
    "        retain_total_predictions, forget_total_predictions = 0, 0\n",
    "\n",
    "        for (retain_inputs, retain_labels), (forget_inputs, forget_labels) in zip(train_dataloader, unlearn_dataloader):\n",
    "            # Move inputs and labels to the specified device\n",
    "            retain_inputs, retain_labels = retain_inputs.to('cuda'), retain_labels.to('cuda')\n",
    "            forget_inputs, forget_labels = forget_inputs.to('cuda'), forget_labels.to('cuda')\n",
    "\n",
    "            # Compute predictions\n",
    "            retain_outputs = model(retain_inputs)\n",
    "            forget_outputs = model(forget_inputs)\n",
    "            _, retain_predictions = torch.max(retain_outputs.data, 1)\n",
    "            _, forget_predictions = torch.max(forget_outputs.data, 1)\n",
    "\n",
    "            # Update the running total of correct predictions and samples\n",
    "            retain_correct_predictions += (retain_predictions == retain_labels).sum().item()\n",
    "            forget_correct_predictions += (forget_predictions == forget_labels).sum().item()\n",
    "            retain_total_predictions += retain_labels.size(0)\n",
    "            forget_total_predictions += forget_labels.size(0)\n",
    "\n",
    "            # Compute the loss and its gradients\n",
    "            retain_loss = criterion(retain_outputs, retain_labels)\n",
    "            forget_loss = criterion(forget_outputs, forget_labels)\n",
    "            retain_running_loss += retain_loss.item()\n",
    "            forget_running_loss += forget_loss.item()\n",
    "\n",
    "            retain_loss.backward()\n",
    "            retain_grads = get_gradient(model)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            (-1.0 * forget_loss).backward()\n",
    "            forget_grads = get_gradient(model)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            rectified_retain_grads, rectified_forget_grads = rectify_graident(retain_grads, forget_grads)\n",
    "            if epoch > 0 and bank.mean_grads(rectified_forget_grads[-1]) != None:\n",
    "                grads, _ = rectify_graident([rectified_forget_grads[-1]], [bank.mean_grads(rectified_forget_grads[-1])])\n",
    "                rectified_forget_grads[-1] = grads[-1]\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                    gamma, epsilon = 100, 0.02\n",
    "                    lambda_weight = 1/(1+torch.exp(gamma*(retain_loss-epsilon)))\n",
    "\n",
    "            for _, param in model.named_parameters():\n",
    "                if param.requires_grad:\n",
    "                    param.grad =  ((1-lambda_weight)*rectified_retain_grads[idx]+lambda_weight*rectified_forget_grads[idx]).view(param.size())\n",
    "                    idx += 1\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Calculate the average loss and accuracy\n",
    "        retain_loss = retain_running_loss / len(train_dataloader)\n",
    "        forget_loss = forget_running_loss / len(train_dataloader)\n",
    "        retain_accuracy = 100 * retain_correct_predictions / retain_total_predictions\n",
    "        forget_accuracy = 100 * forget_correct_predictions / forget_total_predictions\n",
    "        \n",
    "        test_loss, test_accuracy = eval.test(model, test_dataloader, criterion)\n",
    "\n",
    "        pbar.set_postfix(test_loss=test_loss, forget_loss=forget_loss, retain_loss=retain_loss, test_accuracy=test_accuracy, forget_accuracy=forget_accuracy, retain_accuracy=retain_accuracy)\n",
    "\n",
    "        if test_loss > forget_loss:\n",
    "            break\n",
    "\n",
    "run_logger.info(f'GDR-GMA unlearning complete: save_file={SAVE_FILE}, train_loss={train_loss}, train_accuracy={train_accuracy}, test_loss={test_loss}, test_accuracy={test_accuracy}')\n",
    "torch.save(model, SAVE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075e2777",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhessian import hessian\n",
    "\n",
    "HESSIAN_MAX_ITER = 10\n",
    "HESSIAN_TOP_N = 3\n",
    "\n",
    "for blk in model.blocks:\n",
    "    blk.attn.fused_attn = False\n",
    "\n",
    "hessian_comp = hessian(model,\n",
    "                        criterion,\n",
    "                        dataloader=train_dataloader,\n",
    "                        cuda=torch.device('cuda'))\n",
    "\n",
    "top_eigenvalues, _ = hessian_comp.eigenvalues(maxIter=HESSIAN_MAX_ITER, top_n=HESSIAN_TOP_N) # Compute eigenvalues\n",
    "# trace = hessian_comp.trace() # Compute trace\n",
    "# density_eigen, density_weight = hessian_comp.density() # Compute density\n",
    "\n",
    "print(\"Hessian top eigenvalues: \", top_eigenvalues)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
