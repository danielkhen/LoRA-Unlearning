{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044aa744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "TORCH_SEED = 69\n",
    "torch.manual_seed(TORCH_SEED)\n",
    "\n",
    "import logging\n",
    "import os\n",
    "\n",
    "os.makedirs('weights', exist_ok=True)\n",
    "run_logger = logging.getLogger('run_logger')\n",
    "file_handler = logging.FileHandler('weights/runs.log')\n",
    "run_logger.addHandler(file_handler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c11e463",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "NUM_WORKERS = 8\n",
    "BATCH_SIZE = 256\n",
    "INPUT_SIZE = 32\n",
    "NUM_CLASSES = 10\n",
    "dataset = datasets.CIFAR10\n",
    "\n",
    "AUGMENTATIONS = (\n",
    "        transforms.Resize(INPUT_SIZE + INPUT_SIZE // 4),\n",
    "        transforms.RandomResizedCrop(INPUT_SIZE, scale=(0.8, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    ")\n",
    "\n",
    "NORMALIZATIONS = (\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize( (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    ")\n",
    "\n",
    "# Lambda can't be pickled\n",
    "def one_hot_transform(label):\n",
    "    return one_hot(label, NUM_CLASSES).squeeze()\n",
    "\n",
    "TARGET_TRANSFORM = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(one_hot_transform),\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "training_data = dataset(root='./data', train=True, download=True, transform=transforms.Compose(AUGMENTATIONS + NORMALIZATIONS), target_transform=TARGET_TRANSFORM)\n",
    "test_data = dataset(root='./data', train=False, download=True, transform=transforms.Compose(AUGMENTATIONS), target_transform=TARGET_TRANSFORM)\n",
    "\n",
    "# Load data loaders\n",
    "train_dataloader = DataLoader(training_data, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f97ce5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_factory\n",
    "\n",
    "MODEL_NAME = 'context-unet'\n",
    "\n",
    "# Load model\n",
    "model = model_factory.create_model(MODEL_NAME, NUM_CLASSES, input_size=INPUT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5165a821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fine_tuning\n",
    "\n",
    "LORA_RANK = 16\n",
    "LORA_TARGET_REGEXES = ['*']\n",
    "\n",
    "# Apply LoRa\n",
    "model = fine_tuning.get_lora_model(model, LORA_RANK, LORA_TARGET_REGEXES)\n",
    "trainable_params = sum(param.numel() for param in model.parameters() if param.requires_grad)\n",
    "print(f\"Number of trainable parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19a7620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from save file\n",
    "SAVE_FILE = 'weights/model.pth'\n",
    "model = torch.load(SAVE_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42ed156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge and unload LoRa\n",
    "model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad77285",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim, nn\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 0\n",
    "BETA1 = 1e-4\n",
    "BETA2 = 0.02\n",
    "TIMESTEPS = 1000\n",
    "\n",
    "# Load optimizer and criterion\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "b_t = torch.linspace(BETA1, BETA2, TIMESTEPS + 1).to('cuda')\n",
    "a_t = 1 - b_t\n",
    "ab_t = torch.cumprod(a_t, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caac209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from tqdm import tqdm\n",
    "\n",
    "SAVE_FILE = 'weights/model.pth'\n",
    "EPOCHS = 200\n",
    "EVAL_EVERY = 5\n",
    "LABELS_MASK_P = 0.9\n",
    "\n",
    "lr_scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "model.to('cuda')\n",
    "model.train()\n",
    "\n",
    "test_loss = None\n",
    "test_accuracy = None\n",
    "\n",
    "# Normal training\n",
    "with tqdm(range(EPOCHS), unit='epoch', desc='lr=None, train_loss=None') as pbar:\n",
    "    for epoch in pbar:\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, labels in train_dataloader:\n",
    "            # Move inputs and labels to the specified device\n",
    "            inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "\n",
    "            # perturb data\n",
    "            noise = torch.randn_like(inputs, device='cuda')\n",
    "            t = torch.randint(1, TIMESTEPS + 1, (inputs.shape[0], ))\n",
    "            x_pert = ab_t[t].sqrt()[:, None, None, None] * inputs + 1.0 - ab_t[t].sqrt()[:, None, None, None] * noise\n",
    "\n",
    "            # Randomly mask labels\n",
    "            labels = labels*torch.bernoulli(torch.ones((labels.shape[0], 1), device='cuda')*LABELS_MASK_P)\n",
    "\n",
    "            # Predict noise\n",
    "            outputs = model(inputs, t / TIMESTEPS, labels)\n",
    "\n",
    "            # Compute the loss and its gradients\n",
    "            loss = criterion(outputs, noise)\n",
    "            running_loss += loss.item()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Adjust learning weights and zero gradients\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        train_loss = running_loss / len(train_dataloader)\n",
    "\n",
    "        lr_scheduler.step()\n",
    "        lr = lr_scheduler.get_last_lr()[0]\n",
    "\n",
    "        if (epoch + 1) % EVAL_EVERY == 0:\n",
    "            run_logger.debug(f'Normal training checkpoint: save_file={SAVE_FILE}, epoch={epoch}, train_loss={train_loss}')\n",
    "            torch.save(model, f'{SAVE_FILE}.checkpoint')\n",
    "\n",
    "        pbar.set_postfix(lr=lr, train_loss=train_loss)\n",
    "\n",
    "run_logger.info(f'Normal training complete: save_file={SAVE_FILE}, train_loss={train_loss}')\n",
    "torch.save(model, SAVE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75640ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "FORGET_SET_SIZE = 10000\n",
    "\n",
    "training_data = dataset(root='./data', train=True, download=True, transform=transforms.Compose(NORMALIZATIONS))\n",
    "\n",
    "# Class forgetting\n",
    "original_trainset = training_data\n",
    "unlearning_data = Subset(original_trainset, range(FORGET_SET_SIZE))\n",
    "training_data = Subset(original_trainset, range(FORGET_SET_SIZE, len(original_trainset)))\n",
    "\n",
    "# Load data loaders\n",
    "train_dataloader = DataLoader(training_data, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True)\n",
    "unlearn_dataloader = DataLoader(unlearning_data, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455edca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import eval\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "SAVE_FILE = 'weights/modelNG10000.pth'\n",
    "EPOCHS = 200\n",
    "EVAL_EVERY = 5\n",
    "UNLEAN_GRAD_METHOD = 'both' # 'both' or 'ascent\n",
    "\n",
    "model.to('cuda')\n",
    "model.train()\n",
    "\n",
    "test_loss = None\n",
    "test_accuracy = None\n",
    "\n",
    "# NegGrad unlearning\n",
    "with tqdm(range(EPOCHS), unit='epoch', desc='retain_accuracy=None, forget_accuracy=None, test_accuracy=None, retain_loss=None, forget_loss=None, test_loss=None') as pbar:\n",
    "    for epoch in pbar:\n",
    "        retain_running_loss, forget_running_loss = 0.0, 0.0\n",
    "        retain_correct_predictions, forget_correct_predictions = 0, 0\n",
    "        retain_total_predictions, forget_total_predictions = 0, 0\n",
    "\n",
    "        for (retain_inputs, retain_labels), (forget_inputs, forget_labels) in zip(train_dataloader, unlearn_dataloader):\n",
    "            # Move inputs and labels to the specified device\n",
    "            retain_inputs, retain_labels = retain_inputs.to('cuda'), retain_labels.to('cuda')\n",
    "            forget_inputs, forget_labels = forget_inputs.to('cuda'), forget_labels.to('cuda')\n",
    "\n",
    "            # Compute predictions\n",
    "            retain_outputs = model(retain_inputs)\n",
    "            forget_outputs = model(forget_inputs)\n",
    "            _, retain_predictions = torch.max(retain_outputs.data, 1)\n",
    "            _, forget_predictions = torch.max(forget_outputs.data, 1)\n",
    "\n",
    "            # Update the running total of correct predictions and samples\n",
    "            retain_correct_predictions += (retain_predictions == retain_labels).sum().item()\n",
    "            forget_correct_predictions += (forget_predictions == forget_labels).sum().item()\n",
    "            retain_total_predictions += retain_labels.size(0)\n",
    "            forget_total_predictions += forget_labels.size(0)\n",
    "\n",
    "            # Compute the loss and its gradients\n",
    "            retain_loss = criterion(retain_outputs, retain_labels)\n",
    "            forget_loss = criterion(forget_outputs, forget_labels)\n",
    "            retain_running_loss += retain_loss.item()\n",
    "            forget_running_loss += forget_loss.item()\n",
    "            (-1.0 * forget_loss).backward()\n",
    "\n",
    "            if UNLEAN_GRAD_METHOD == 'both':\n",
    "                retain_loss.backward()\n",
    "\n",
    "            # Adjust learning weights and zero gradients\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Calculate the average loss and accuracy\n",
    "        retain_loss = retain_running_loss / len(train_dataloader)\n",
    "        forget_loss = forget_running_loss / len(train_dataloader)\n",
    "        retain_accuracy = 100 * retain_correct_predictions / retain_total_predictions\n",
    "        forget_accuracy = 100 * forget_correct_predictions / forget_total_predictions\n",
    "        \n",
    "        test_loss, test_accuracy = eval.test(model, test_dataloader, criterion)\n",
    "\n",
    "        pbar.set_postfix(test_loss=test_loss, forget_loss=forget_loss, retain_loss=retain_loss, test_accuracy=test_accuracy, forget_accuracy=forget_accuracy, retain_accuracy=retain_accuracy)\n",
    "\n",
    "        if test_loss > forget_loss:\n",
    "            break\n",
    "\n",
    "run_logger.info(f'NegGrad unlearning complete: save_file={SAVE_FILE}, train_loss={train_loss}, train_accuracy={train_accuracy}, test_loss={test_loss}, test_accuracy={test_accuracy}')\n",
    "torch.save(model, SAVE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b161eb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import eval\n",
    "\n",
    "from orthograd import OrthogonalGrad, AdamUpdateDirection\n",
    "from tqdm import tqdm\n",
    "\n",
    "SAVE_FILE = 'weights/modelOG10000.pth'\n",
    "EPOCHS = 200\n",
    "EVAL_EVERY = 5\n",
    "\n",
    "model.to('cuda')\n",
    "model.train()\n",
    "\n",
    "test_loss = None\n",
    "test_accuracy = None\n",
    "\n",
    "optimizer_retain = AdamUpdateDirection(model.parameters())\n",
    "optimizer_forget = AdamUpdateDirection(model.parameters())\n",
    "\n",
    "unlearn_method = OrthogonalGrad(\n",
    "    lr=LEARNING_RATE,\n",
    "    loss=criterion,\n",
    "    optimizer_retain=optimizer_retain,\n",
    "    optimizer_unlearn=optimizer_forget,\n",
    "    retain_grad_mode='per_sample',\n",
    "    update_mode='both',\n",
    "    original_model=model,\n",
    "    grad_mask=None,\n",
    "    alpha=0.5,\n",
    ")\n",
    "\n",
    "# NegGrad unlearning\n",
    "with tqdm(range(EPOCHS), unit='epoch', desc='retain_accuracy=None, forget_accuracy=None, test_accuracy=None, retain_loss=None, forget_loss=None, test_loss=None') as pbar:\n",
    "    for epoch in pbar:\n",
    "        retain_running_loss, forget_running_loss = 0.0, 0.0\n",
    "        retain_correct_predictions, forget_correct_predictions = 0, 0\n",
    "        retain_total_predictions, forget_total_predictions = 0, 0\n",
    "        model.train() # Set the model to training mode\n",
    "\n",
    "        for (retain_inputs, retain_labels), (forget_inputs, forget_labels) in zip(train_dataloader, unlearn_dataloader):\n",
    "            # Move inputs and labels to the specified device\n",
    "            retain_inputs, retain_labels = retain_inputs.to('cuda'), retain_labels.to('cuda')\n",
    "            forget_inputs, forget_labels = forget_inputs.to('cuda'), forget_labels.to('cuda')\n",
    "\n",
    "            # Compute predictions\n",
    "            retain_outputs = model(retain_inputs)\n",
    "            forget_outputs = model(forget_inputs)\n",
    "            _, retain_predictions = torch.max(retain_outputs.data, 1)\n",
    "            _, forget_predictions = torch.max(forget_outputs.data, 1)\n",
    "\n",
    "            # Update the running total of correct predictions and samples\n",
    "            retain_correct_predictions += (retain_predictions == retain_labels).sum().item()\n",
    "            forget_correct_predictions += (forget_predictions == forget_labels).sum().item()\n",
    "            retain_total_predictions += retain_labels.size(0)\n",
    "            forget_total_predictions += forget_labels.size(0)\n",
    "\n",
    "            # Compute the loss and its gradients\n",
    "            forget_loss, retain_loss = unlearn_method(\n",
    "                model, forget_inputs, forget_labels, retain_inputs, retain_labels\n",
    "            )\n",
    "\n",
    "            retain_running_loss += retain_loss.item()\n",
    "            forget_running_loss += forget_loss.item()\n",
    "\n",
    "        # Calculate the average loss and accuracy\n",
    "        retain_avg_loss = retain_running_loss / len(train_dataloader)\n",
    "        forget_avg_loss = forget_running_loss / len(train_dataloader)\n",
    "        retain_accuracy = 100 * retain_correct_predictions / retain_total_predictions\n",
    "        forget_accuracy = 100 * forget_correct_predictions / forget_total_predictions\n",
    "        \n",
    "        test_loss, test_accuracy = eval.test(model, test_dataloader, criterion)\n",
    "\n",
    "        pbar.set_postfix(test_loss=test_loss, forget_loss=forget_loss, retain_loss=retain_loss, test_accuracy=test_accuracy, forget_accuracy=forget_accuracy, retain_accuracy=retain_accuracy)\n",
    "        \n",
    "        if test_loss > forget_loss:\n",
    "            break\n",
    "\n",
    "run_logger.info(f'OrthoGrad unlearning complete: save_file={SAVE_FILE}, train_loss={train_loss}, train_accuracy={train_accuracy}, test_loss={test_loss}, test_accuracy={test_accuracy}')\n",
    "torch.save(model, SAVE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb13f8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import eval\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "from gdr_gma import MemoryBank, get_gradient, rectify_graident\n",
    "\n",
    "SAVE_FILE = 'weights/modelGG10000.pth'\n",
    "EPOCHS = 200\n",
    "EVAL_EVERY = 5\n",
    "\n",
    "model.to('cuda')\n",
    "model.train()\n",
    "\n",
    "test_loss = None\n",
    "test_accuracy = None\n",
    "\n",
    "batches = math.ceil(len(zip(train_dataloader, unlearn_dataloader)) / BATCH_SIZE)\n",
    "bank = MemoryBank(size=batches)\n",
    "\n",
    "# NegGrad unlearning\n",
    "with tqdm(range(EPOCHS), unit='epoch', desc='retain_accuracy=None, forget_accuracy=None, test_accuracy=None, retain_loss=None, forget_loss=None, test_loss=None') as pbar:\n",
    "    for epoch in pbar:\n",
    "        retain_running_loss, forget_running_loss = 0.0, 0.0\n",
    "        retain_correct_predictions, forget_correct_predictions = 0, 0\n",
    "        retain_total_predictions, forget_total_predictions = 0, 0\n",
    "\n",
    "        for (retain_inputs, retain_labels), (forget_inputs, forget_labels) in zip(train_dataloader, unlearn_dataloader):\n",
    "            # Move inputs and labels to the specified device\n",
    "            retain_inputs, retain_labels = retain_inputs.to('cuda'), retain_labels.to('cuda')\n",
    "            forget_inputs, forget_labels = forget_inputs.to('cuda'), forget_labels.to('cuda')\n",
    "\n",
    "            # Compute predictions\n",
    "            retain_outputs = model(retain_inputs)\n",
    "            forget_outputs = model(forget_inputs)\n",
    "            _, retain_predictions = torch.max(retain_outputs.data, 1)\n",
    "            _, forget_predictions = torch.max(forget_outputs.data, 1)\n",
    "\n",
    "            # Update the running total of correct predictions and samples\n",
    "            retain_correct_predictions += (retain_predictions == retain_labels).sum().item()\n",
    "            forget_correct_predictions += (forget_predictions == forget_labels).sum().item()\n",
    "            retain_total_predictions += retain_labels.size(0)\n",
    "            forget_total_predictions += forget_labels.size(0)\n",
    "\n",
    "            # Compute the loss and its gradients\n",
    "            retain_loss = criterion(retain_outputs, retain_labels)\n",
    "            forget_loss = criterion(forget_outputs, forget_labels)\n",
    "            retain_running_loss += retain_loss.item()\n",
    "            forget_running_loss += forget_loss.item()\n",
    "\n",
    "            retain_loss.backward()\n",
    "            retain_grads = get_gradient(model)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            (-1.0 * forget_loss).backward()\n",
    "            forget_grads = get_gradient(model)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            rectified_retain_grads, rectified_forget_grads = rectify_graident(retain_grads, forget_grads)\n",
    "            if epoch > 0 and bank.mean_grads(rectified_forget_grads[-1]) != None:\n",
    "                grads, _ = rectify_graident([rectified_forget_grads[-1]], [bank.mean_grads(rectified_forget_grads[-1])])\n",
    "                rectified_forget_grads[-1] = grads[-1]\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                    gamma, epsilon = 100, 0.02\n",
    "                    lambda_weight = 1/(1+torch.exp(gamma*(retain_loss-epsilon)))\n",
    "\n",
    "            for _, param in model.named_parameters():\n",
    "                if param.requires_grad:\n",
    "                    param.grad =  ((1-lambda_weight)*rectified_retain_grads[idx]+lambda_weight*rectified_forget_grads[idx]).view(param.size())\n",
    "                    idx += 1\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Calculate the average loss and accuracy\n",
    "        retain_loss = retain_running_loss / len(train_dataloader)\n",
    "        forget_loss = forget_running_loss / len(train_dataloader)\n",
    "        retain_accuracy = 100 * retain_correct_predictions / retain_total_predictions\n",
    "        forget_accuracy = 100 * forget_correct_predictions / forget_total_predictions\n",
    "        \n",
    "        test_loss, test_accuracy = eval.test(model, test_dataloader, criterion)\n",
    "\n",
    "        pbar.set_postfix(test_loss=test_loss, forget_loss=forget_loss, retain_loss=retain_loss, test_accuracy=test_accuracy, forget_accuracy=forget_accuracy, retain_accuracy=retain_accuracy)\n",
    "\n",
    "        if test_loss > forget_loss:\n",
    "            break\n",
    "\n",
    "run_logger.info(f'GDR-GMA unlearning complete: save_file={SAVE_FILE}, train_loss={train_loss}, train_accuracy={train_accuracy}, test_loss={test_loss}, test_accuracy={test_accuracy}')\n",
    "torch.save(model, SAVE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075e2777",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhessian import hessian\n",
    "\n",
    "HESSIAN_MAX_ITER = 10\n",
    "HESSIAN_TOP_N = 3\n",
    "\n",
    "for blk in model.blocks:\n",
    "    blk.attn.fused_attn = False\n",
    "\n",
    "hessian_comp = hessian(model,\n",
    "                        criterion,\n",
    "                        dataloader=train_dataloader,\n",
    "                        cuda=torch.device('cuda'))\n",
    "\n",
    "top_eigenvalues, _ = hessian_comp.eigenvalues(maxIter=HESSIAN_MAX_ITER, top_n=HESSIAN_TOP_N) # Compute eigenvalues\n",
    "# trace = hessian_comp.trace() # Compute trace\n",
    "# density_eigen, density_weight = hessian_comp.density() # Compute density\n",
    "\n",
    "print(\"Hessian top eigenvalues: \", top_eigenvalues)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
