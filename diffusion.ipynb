{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "044aa744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "TORCH_SEED = 69\n",
    "torch.manual_seed(TORCH_SEED)\n",
    "\n",
    "import logging\n",
    "import os\n",
    "\n",
    "os.makedirs('weights', exist_ok=True)\n",
    "run_logger = logging.getLogger('run_logger')\n",
    "file_handler = logging.FileHandler('weights/runs.log')\n",
    "run_logger.addHandler(file_handler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c11e463",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "NUM_WORKERS = 8\n",
    "BATCH_SIZE = 256\n",
    "INPUT_SIZE = 64\n",
    "NUM_CLASSES = 10\n",
    "dataset = datasets.CIFAR10\n",
    "\n",
    "AUGMENTATIONS = (\n",
    "        transforms.Resize(INPUT_SIZE + INPUT_SIZE // 4),\n",
    "        transforms.RandomResizedCrop(INPUT_SIZE, scale=(0.8, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    ")\n",
    "\n",
    "NORMALIZATIONS = (\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize( (0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    ")\n",
    "\n",
    "# Load datasets\n",
    "training_data = dataset(root='./data', train=True, download=True, transform=transforms.Compose(AUGMENTATIONS + NORMALIZATIONS))\n",
    "\n",
    "# Load data loaders\n",
    "train_dataloader = DataLoader(training_data, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f97ce5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_factory\n",
    "\n",
    "MODEL_NAME = 'context-unet'\n",
    "\n",
    "# Load model\n",
    "model = model_factory.create_model(MODEL_NAME, NUM_CLASSES, input_size=INPUT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5165a821",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Target modules set() not found in the base model. Please check the target modules and try again.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m LORA_TARGET_REGEXES \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mup0*\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Apply LoRa\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mfine_tuning\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_lora_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLORA_RANK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLORA_TARGET_REGEXES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m trainable_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(param\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters() \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of trainable parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainable_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\danie\\lora-unlearning\\fine_tuning.py:22\u001b[0m, in \u001b[0;36mget_lora_model\u001b[1;34m(model, rank, target_regexes)\u001b[0m\n\u001b[0;32m     16\u001b[0m modules_to_save \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     17\u001b[0m     name \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_modules()\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m target_modules \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(module) \u001b[38;5;129;01min\u001b[39;00m LORA_LAYERS\n\u001b[0;32m     19\u001b[0m ]\n\u001b[0;32m     21\u001b[0m lora_config \u001b[38;5;241m=\u001b[39m LoraConfig(r\u001b[38;5;241m=\u001b[39mrank, target_modules\u001b[38;5;241m=\u001b[39mtarget_modules, modules_to_save\u001b[38;5;241m=\u001b[39mmodules_to_save)\n\u001b[1;32m---> 22\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mget_peft_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\peft\\mapping.py:189\u001b[0m, in \u001b[0;36mget_peft_model\u001b[1;34m(model, peft_config, adapter_name, mixed, autocast_adapter_dtype, revision)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PeftMixedModel(model, peft_config, adapter_name\u001b[38;5;241m=\u001b[39madapter_name)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mtask_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m MODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mis_prompt_learning:\n\u001b[1;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPeftModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mis_prompt_learning:\n\u001b[0;32m    192\u001b[0m     peft_config \u001b[38;5;241m=\u001b[39m _prepare_prompt_learning_config(peft_config, model_config)\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\peft\\peft_model.py:171\u001b[0m, in \u001b[0;36mPeftModel.__init__\u001b[1;34m(self, model, peft_config, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage)\u001b[0m\n\u001b[0;32m    169\u001b[0m     ctx \u001b[38;5;241m=\u001b[39m init_empty_weights \u001b[38;5;28;01mif\u001b[39;00m low_cpu_mem_usage \u001b[38;5;28;01melse\u001b[39;00m nullcontext\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx():\n\u001b[1;32m--> 171\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_additional_trainable_modules(peft_config, adapter_name)\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cast_adapter_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\peft\\tuners\\lora\\model.py:141\u001b[0m, in \u001b[0;36mLoraModel.__init__\u001b[1;34m(self, model, config, adapter_name, low_cpu_mem_usage)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, config, adapter_name, low_cpu_mem_usage: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 141\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:184\u001b[0m, in \u001b[0;36mBaseTuner.__init__\u001b[1;34m(self, model, peft_config, adapter_name, low_cpu_mem_usage)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pre_injection_hook(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config[adapter_name], adapter_name)\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft_config \u001b[38;5;241m!=\u001b[39m PeftType\u001b[38;5;241m.\u001b[39mXLORA \u001b[38;5;129;01mor\u001b[39;00m peft_config[adapter_name] \u001b[38;5;241m!=\u001b[39m PeftType\u001b[38;5;241m.\u001b[39mXLORA:\n\u001b[1;32m--> 184\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minject_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# Copy the peft_config in the injected model.\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpeft_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:509\u001b[0m, in \u001b[0;36mBaseTuner.inject_adapter\u001b[1;34m(self, model, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;66;03m# Handle X-LoRA case.\u001b[39;00m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_target_modules_in_base_model \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(peft_config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_modules\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 509\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    510\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget modules \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpeft_config\u001b[38;5;241m.\u001b[39mtarget_modules\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in the base model. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    511\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check the target modules and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    512\u001b[0m     )\n\u001b[0;32m    514\u001b[0m \u001b[38;5;66;03m# It's important to set the adapter here (again), because otherwise it can happen that if a 2nd adapter is\u001b[39;00m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;66;03m# added, and it targets different layer(s) than the first adapter (which is active), then those different\u001b[39;00m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;66;03m# layers will be activated, which we don't want.\u001b[39;00m\n\u001b[0;32m    517\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_adapter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapters)\n",
      "\u001b[1;31mValueError\u001b[0m: Target modules set() not found in the base model. Please check the target modules and try again."
     ]
    }
   ],
   "source": [
    "import fine_tuning\n",
    "\n",
    "LORA_RANK = 16\n",
    "LORA_TARGET_REGEXES = ['up0.*']\n",
    "\n",
    "# Apply LoRa\n",
    "model = fine_tuning.get_lora_model(model, LORA_RANK, LORA_TARGET_REGEXES)\n",
    "trainable_params = sum(param.numel() for param in model.parameters() if param.requires_grad)\n",
    "print(f\"Number of trainable parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19a7620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from save file\n",
    "SAVE_FILE = 'weights/model.pth'\n",
    "model = torch.load(SAVE_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42ed156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge and unload LoRa\n",
    "model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad77285",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim, nn\n",
    "\n",
    "LEARNING_RATE = 3e-4\n",
    "WEIGHT_DECAY = 0\n",
    "BETA1 = 1e-4\n",
    "BETA2 = 0.02\n",
    "TIMESTEPS = 1000\n",
    "\n",
    "# Load optimizer and criterion\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "b_t = torch.linspace(BETA1, BETA2, TIMESTEPS + 1).to('cuda')\n",
    "a_t = 1 - b_t\n",
    "ab_t = torch.cumprod(a_t, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caac209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from tqdm import tqdm\n",
    "\n",
    "SAVE_FILE = 'weights/model.pth'\n",
    "EPOCHS = 200\n",
    "EVAL_EVERY = 5\n",
    "CFG_PROB=0.1\n",
    "\n",
    "lr_scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "model.to('cuda')\n",
    "model.train()\n",
    "\n",
    "test_loss = None\n",
    "test_accuracy = None\n",
    "\n",
    "# Normal training\n",
    "with tqdm(range(EPOCHS), unit='epoch', desc='lr=None, train_loss=None') as pbar:\n",
    "    for epoch in pbar:\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, labels in train_dataloader:\n",
    "            # Move inputs and labels to the specified device\n",
    "            inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "\n",
    "            # perturb data\n",
    "            noise = torch.randn_like(inputs, device='cuda')\n",
    "            t = torch.randint(1, TIMESTEPS + 1, (inputs.shape[0], ), device='cuda')\n",
    "            sqrt_ab_t = ab_t[t].sqrt().view(-1, 1, 1, 1)\n",
    "            sqrt_one_minus_ab_t = (1 - ab_t[t]).sqrt().view(-1, 1, 1, 1)\n",
    "            x_pert = sqrt_ab_t * inputs + sqrt_one_minus_ab_t * noise\n",
    "            \n",
    "            # Apply classifier-free guidance: randomly drop labels\n",
    "            keep_mask = (torch.rand(labels.shape, device='cuda') > CFG_PROB)\n",
    "            labels = labels * keep_mask + 0 * (~keep_mask)\n",
    "\n",
    "            # Predict noise\n",
    "            outputs = model(x_pert, t, labels)\n",
    "\n",
    "            # Compute the loss and its gradients\n",
    "            loss = criterion(outputs, noise)\n",
    "            running_loss += loss.item()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Adjust learning weights and zero gradients\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        train_loss = running_loss / len(train_dataloader)\n",
    "\n",
    "        lr_scheduler.step()\n",
    "        lr = lr_scheduler.get_last_lr()[0]\n",
    "\n",
    "        if (epoch + 1) % EVAL_EVERY == 0:\n",
    "            run_logger.debug(f'Normal training checkpoint: save_file={SAVE_FILE}, epoch={epoch}, train_loss={train_loss}')\n",
    "            torch.save(model, f'{SAVE_FILE}.checkpoint')\n",
    "\n",
    "        pbar.set_postfix(lr=lr, train_loss=train_loss)\n",
    "\n",
    "run_logger.info(f'Normal training complete: save_file={SAVE_FILE}, train_loss={train_loss}')\n",
    "torch.save(model, SAVE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a958f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Number of samples to generate\n",
    "num_samples = 8\n",
    "\n",
    "# Sampling parameters\n",
    "with torch.no_grad():\n",
    "    # Start from pure noise\n",
    "    x = torch.randn(num_samples, 3, INPUT_SIZE, INPUT_SIZE).to('cuda')\n",
    "    labels = torch.zeros(num_samples, NUM_CLASSES).to('cuda')  # or use one-hot for conditional generation\n",
    "\n",
    "    for t in reversed(range(1, TIMESTEPS + 1)):\n",
    "        t_tensor = torch.full((num_samples,), t, device='cuda', dtype=torch.long)\n",
    "        t_norm = t_tensor / TIMESTEPS\n",
    "\n",
    "        # Predict noise\n",
    "        pred_noise = model(x, t_norm, labels)\n",
    "\n",
    "        # Compute coefficients\n",
    "        beta_t = b_t[t].view(-1, 1, 1, 1)\n",
    "        alpha_t = a_t[t].view(-1, 1, 1, 1)\n",
    "        ab_t_ = ab_t[t].view(-1, 1, 1, 1)\n",
    "\n",
    "        # DDPM sampling step\n",
    "        if t > 1:\n",
    "            noise = torch.randn_like(x)\n",
    "        else:\n",
    "            noise = torch.zeros_like(x)\n",
    "        x = (1 / alpha_t.sqrt()) * (x - (beta_t / (1 - ab_t_).sqrt()) * pred_noise) + beta_t.sqrt() * noise\n",
    "\n",
    "    # Convert to numpy and denormalize for visualization\n",
    "    samples = x.clamp(-1, 1).cpu().numpy()\n",
    "    samples = (samples * 0.5) + 0.5  # if your data was normalized to [-1, 1]\n",
    "\n",
    "    # Plot samples\n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(num_samples * 2, 2))\n",
    "    for i, ax in enumerate(axes):\n",
    "        img = np.transpose(samples[i], (1, 2, 0))\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75640ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "FORGET_SET_SIZE = 10000\n",
    "\n",
    "training_data = dataset(root='./data', train=True, download=True, transform=transforms.Compose(NORMALIZATIONS))\n",
    "\n",
    "# Class forgetting\n",
    "original_trainset = training_data\n",
    "unlearning_data = Subset(original_trainset, range(FORGET_SET_SIZE))\n",
    "training_data = Subset(original_trainset, range(FORGET_SET_SIZE, len(original_trainset)))\n",
    "\n",
    "# Load data loaders\n",
    "train_dataloader = DataLoader(training_data, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True)\n",
    "unlearn_dataloader = DataLoader(unlearning_data, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455edca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import eval\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "SAVE_FILE = 'weights/modelNG10000.pth'\n",
    "EPOCHS = 200\n",
    "EVAL_EVERY = 5\n",
    "UNLEAN_GRAD_METHOD = 'both' # 'both' or 'ascent\n",
    "\n",
    "model.to('cuda')\n",
    "model.train()\n",
    "\n",
    "test_loss = None\n",
    "test_accuracy = None\n",
    "\n",
    "# NegGrad unlearning\n",
    "with tqdm(range(EPOCHS), unit='epoch', desc='retain_accuracy=None, forget_accuracy=None, test_accuracy=None, retain_loss=None, forget_loss=None, test_loss=None') as pbar:\n",
    "    for epoch in pbar:\n",
    "        retain_running_loss, forget_running_loss = 0.0, 0.0\n",
    "        retain_correct_predictions, forget_correct_predictions = 0, 0\n",
    "        retain_total_predictions, forget_total_predictions = 0, 0\n",
    "\n",
    "        for (retain_inputs, retain_labels), (forget_inputs, forget_labels) in zip(train_dataloader, unlearn_dataloader):\n",
    "            # Move inputs and labels to the specified device\n",
    "            retain_inputs, retain_labels = retain_inputs.to('cuda'), retain_labels.to('cuda')\n",
    "            forget_inputs, forget_labels = forget_inputs.to('cuda'), forget_labels.to('cuda')\n",
    "\n",
    "            # Compute predictions\n",
    "            retain_outputs = model(retain_inputs)\n",
    "            forget_outputs = model(forget_inputs)\n",
    "            _, retain_predictions = torch.max(retain_outputs.data, 1)\n",
    "            _, forget_predictions = torch.max(forget_outputs.data, 1)\n",
    "\n",
    "            # Update the running total of correct predictions and samples\n",
    "            retain_correct_predictions += (retain_predictions == retain_labels).sum().item()\n",
    "            forget_correct_predictions += (forget_predictions == forget_labels).sum().item()\n",
    "            retain_total_predictions += retain_labels.size(0)\n",
    "            forget_total_predictions += forget_labels.size(0)\n",
    "\n",
    "            # Compute the loss and its gradients\n",
    "            retain_loss = criterion(retain_outputs, retain_labels)\n",
    "            forget_loss = criterion(forget_outputs, forget_labels)\n",
    "            retain_running_loss += retain_loss.item()\n",
    "            forget_running_loss += forget_loss.item()\n",
    "            (-1.0 * forget_loss).backward()\n",
    "\n",
    "            if UNLEAN_GRAD_METHOD == 'both':\n",
    "                retain_loss.backward()\n",
    "\n",
    "            # Adjust learning weights and zero gradients\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Calculate the average loss and accuracy\n",
    "        retain_loss = retain_running_loss / len(train_dataloader)\n",
    "        forget_loss = forget_running_loss / len(train_dataloader)\n",
    "        retain_accuracy = 100 * retain_correct_predictions / retain_total_predictions\n",
    "        forget_accuracy = 100 * forget_correct_predictions / forget_total_predictions\n",
    "        \n",
    "        test_loss, test_accuracy = eval.test(model, test_dataloader, criterion)\n",
    "\n",
    "        pbar.set_postfix(test_loss=test_loss, forget_loss=forget_loss, retain_loss=retain_loss, test_accuracy=test_accuracy, forget_accuracy=forget_accuracy, retain_accuracy=retain_accuracy)\n",
    "\n",
    "        if test_loss > forget_loss:\n",
    "            break\n",
    "\n",
    "run_logger.info(f'NegGrad unlearning complete: save_file={SAVE_FILE}, train_loss={train_loss}, train_accuracy={train_accuracy}, test_loss={test_loss}, test_accuracy={test_accuracy}')\n",
    "torch.save(model, SAVE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b161eb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import eval\n",
    "\n",
    "from orthograd import OrthogonalGrad, AdamUpdateDirection\n",
    "from tqdm import tqdm\n",
    "\n",
    "SAVE_FILE = 'weights/modelOG10000.pth'\n",
    "EPOCHS = 200\n",
    "EVAL_EVERY = 5\n",
    "\n",
    "model.to('cuda')\n",
    "model.train()\n",
    "\n",
    "test_loss = None\n",
    "test_accuracy = None\n",
    "\n",
    "optimizer_retain = AdamUpdateDirection(model.parameters())\n",
    "optimizer_forget = AdamUpdateDirection(model.parameters())\n",
    "\n",
    "unlearn_method = OrthogonalGrad(\n",
    "    lr=LEARNING_RATE,\n",
    "    loss=criterion,\n",
    "    optimizer_retain=optimizer_retain,\n",
    "    optimizer_unlearn=optimizer_forget,\n",
    "    retain_grad_mode='per_sample',\n",
    "    update_mode='both',\n",
    "    original_model=model,\n",
    "    grad_mask=None,\n",
    "    alpha=0.5,\n",
    ")\n",
    "\n",
    "# NegGrad unlearning\n",
    "with tqdm(range(EPOCHS), unit='epoch', desc='retain_accuracy=None, forget_accuracy=None, test_accuracy=None, retain_loss=None, forget_loss=None, test_loss=None') as pbar:\n",
    "    for epoch in pbar:\n",
    "        retain_running_loss, forget_running_loss = 0.0, 0.0\n",
    "        retain_correct_predictions, forget_correct_predictions = 0, 0\n",
    "        retain_total_predictions, forget_total_predictions = 0, 0\n",
    "        model.train() # Set the model to training mode\n",
    "\n",
    "        for (retain_inputs, retain_labels), (forget_inputs, forget_labels) in zip(train_dataloader, unlearn_dataloader):\n",
    "            # Move inputs and labels to the specified device\n",
    "            retain_inputs, retain_labels = retain_inputs.to('cuda'), retain_labels.to('cuda')\n",
    "            forget_inputs, forget_labels = forget_inputs.to('cuda'), forget_labels.to('cuda')\n",
    "\n",
    "            # Compute predictions\n",
    "            retain_outputs = model(retain_inputs)\n",
    "            forget_outputs = model(forget_inputs)\n",
    "            _, retain_predictions = torch.max(retain_outputs.data, 1)\n",
    "            _, forget_predictions = torch.max(forget_outputs.data, 1)\n",
    "\n",
    "            # Update the running total of correct predictions and samples\n",
    "            retain_correct_predictions += (retain_predictions == retain_labels).sum().item()\n",
    "            forget_correct_predictions += (forget_predictions == forget_labels).sum().item()\n",
    "            retain_total_predictions += retain_labels.size(0)\n",
    "            forget_total_predictions += forget_labels.size(0)\n",
    "\n",
    "            # Compute the loss and its gradients\n",
    "            forget_loss, retain_loss = unlearn_method(\n",
    "                model, forget_inputs, forget_labels, retain_inputs, retain_labels\n",
    "            )\n",
    "\n",
    "            retain_running_loss += retain_loss.item()\n",
    "            forget_running_loss += forget_loss.item()\n",
    "\n",
    "        # Calculate the average loss and accuracy\n",
    "        retain_avg_loss = retain_running_loss / len(train_dataloader)\n",
    "        forget_avg_loss = forget_running_loss / len(train_dataloader)\n",
    "        retain_accuracy = 100 * retain_correct_predictions / retain_total_predictions\n",
    "        forget_accuracy = 100 * forget_correct_predictions / forget_total_predictions\n",
    "        \n",
    "        test_loss, test_accuracy = eval.test(model, test_dataloader, criterion)\n",
    "\n",
    "        pbar.set_postfix(test_loss=test_loss, forget_loss=forget_loss, retain_loss=retain_loss, test_accuracy=test_accuracy, forget_accuracy=forget_accuracy, retain_accuracy=retain_accuracy)\n",
    "        \n",
    "        if test_loss > forget_loss:\n",
    "            break\n",
    "\n",
    "run_logger.info(f'OrthoGrad unlearning complete: save_file={SAVE_FILE}, train_loss={train_loss}, train_accuracy={train_accuracy}, test_loss={test_loss}, test_accuracy={test_accuracy}')\n",
    "torch.save(model, SAVE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb13f8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import eval\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "from gdr_gma import MemoryBank, get_gradient, rectify_graident\n",
    "\n",
    "SAVE_FILE = 'weights/modelGG10000.pth'\n",
    "EPOCHS = 200\n",
    "EVAL_EVERY = 5\n",
    "\n",
    "model.to('cuda')\n",
    "model.train()\n",
    "\n",
    "test_loss = None\n",
    "test_accuracy = None\n",
    "\n",
    "batches = math.ceil(len(zip(train_dataloader, unlearn_dataloader)) / BATCH_SIZE)\n",
    "bank = MemoryBank(size=batches)\n",
    "\n",
    "# NegGrad unlearning\n",
    "with tqdm(range(EPOCHS), unit='epoch', desc='retain_accuracy=None, forget_accuracy=None, test_accuracy=None, retain_loss=None, forget_loss=None, test_loss=None') as pbar:\n",
    "    for epoch in pbar:\n",
    "        retain_running_loss, forget_running_loss = 0.0, 0.0\n",
    "        retain_correct_predictions, forget_correct_predictions = 0, 0\n",
    "        retain_total_predictions, forget_total_predictions = 0, 0\n",
    "\n",
    "        for (retain_inputs, retain_labels), (forget_inputs, forget_labels) in zip(train_dataloader, unlearn_dataloader):\n",
    "            # Move inputs and labels to the specified device\n",
    "            retain_inputs, retain_labels = retain_inputs.to('cuda'), retain_labels.to('cuda')\n",
    "            forget_inputs, forget_labels = forget_inputs.to('cuda'), forget_labels.to('cuda')\n",
    "\n",
    "            # Compute predictions\n",
    "            retain_outputs = model(retain_inputs)\n",
    "            forget_outputs = model(forget_inputs)\n",
    "            _, retain_predictions = torch.max(retain_outputs.data, 1)\n",
    "            _, forget_predictions = torch.max(forget_outputs.data, 1)\n",
    "\n",
    "            # Update the running total of correct predictions and samples\n",
    "            retain_correct_predictions += (retain_predictions == retain_labels).sum().item()\n",
    "            forget_correct_predictions += (forget_predictions == forget_labels).sum().item()\n",
    "            retain_total_predictions += retain_labels.size(0)\n",
    "            forget_total_predictions += forget_labels.size(0)\n",
    "\n",
    "            # Compute the loss and its gradients\n",
    "            retain_loss = criterion(retain_outputs, retain_labels)\n",
    "            forget_loss = criterion(forget_outputs, forget_labels)\n",
    "            retain_running_loss += retain_loss.item()\n",
    "            forget_running_loss += forget_loss.item()\n",
    "\n",
    "            retain_loss.backward()\n",
    "            retain_grads = get_gradient(model)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            (-1.0 * forget_loss).backward()\n",
    "            forget_grads = get_gradient(model)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            rectified_retain_grads, rectified_forget_grads = rectify_graident(retain_grads, forget_grads)\n",
    "            if epoch > 0 and bank.mean_grads(rectified_forget_grads[-1]) != None:\n",
    "                grads, _ = rectify_graident([rectified_forget_grads[-1]], [bank.mean_grads(rectified_forget_grads[-1])])\n",
    "                rectified_forget_grads[-1] = grads[-1]\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                    gamma, epsilon = 100, 0.02\n",
    "                    lambda_weight = 1/(1+torch.exp(gamma*(retain_loss-epsilon)))\n",
    "\n",
    "            for _, param in model.named_parameters():\n",
    "                if param.requires_grad:\n",
    "                    param.grad =  ((1-lambda_weight)*rectified_retain_grads[idx]+lambda_weight*rectified_forget_grads[idx]).view(param.size())\n",
    "                    idx += 1\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Calculate the average loss and accuracy\n",
    "        retain_loss = retain_running_loss / len(train_dataloader)\n",
    "        forget_loss = forget_running_loss / len(train_dataloader)\n",
    "        retain_accuracy = 100 * retain_correct_predictions / retain_total_predictions\n",
    "        forget_accuracy = 100 * forget_correct_predictions / forget_total_predictions\n",
    "        \n",
    "        test_loss, test_accuracy = eval.test(model, test_dataloader, criterion)\n",
    "\n",
    "        pbar.set_postfix(test_loss=test_loss, forget_loss=forget_loss, retain_loss=retain_loss, test_accuracy=test_accuracy, forget_accuracy=forget_accuracy, retain_accuracy=retain_accuracy)\n",
    "\n",
    "        if test_loss > forget_loss:\n",
    "            break\n",
    "\n",
    "run_logger.info(f'GDR-GMA unlearning complete: save_file={SAVE_FILE}, train_loss={train_loss}, train_accuracy={train_accuracy}, test_loss={test_loss}, test_accuracy={test_accuracy}')\n",
    "torch.save(model, SAVE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075e2777",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhessian import hessian\n",
    "\n",
    "HESSIAN_MAX_ITER = 10\n",
    "HESSIAN_TOP_N = 3\n",
    "\n",
    "for blk in model.blocks:\n",
    "    blk.attn.fused_attn = False\n",
    "\n",
    "hessian_comp = hessian(model,\n",
    "                        criterion,\n",
    "                        dataloader=train_dataloader,\n",
    "                        cuda=torch.device('cuda'))\n",
    "\n",
    "top_eigenvalues, _ = hessian_comp.eigenvalues(maxIter=HESSIAN_MAX_ITER, top_n=HESSIAN_TOP_N) # Compute eigenvalues\n",
    "# trace = hessian_comp.trace() # Compute trace\n",
    "# density_eigen, density_weight = hessian_comp.density() # Compute density\n",
    "\n",
    "print(\"Hessian top eigenvalues: \", top_eigenvalues)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
